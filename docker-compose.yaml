version: "3.8"

services:
  mongodb:
    image: mongo:7
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin123
    volumes:
      - ./mongodb_data:/data/db
      - ./queries:/queries
    networks:
      - mongodb_platform

  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./infra/minio-storage:/data
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio@123
    command: server --console-address ":9001" /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 5s
    networks:
      - mongodb_platform

  postgres-metastore:
    image: postgres:14
    container_name: postgres-metastore
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
      POSTGRES_HOST_AUTH_METHOD: password
    volumes:
      - ./infra/postgres-init:/docker-entrypoint-initdb.d
      - ./infra/pg-metastore-data:/var/lib/postgresql/data
    ports:
      - "15432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "hive"]
      interval: 10s
      retries: 5
      start_period: 5s
      timeout: 5s
    networks:
      - mongodb_platform

  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    depends_on:
      postgres-metastore:
        condition: service_healthy
      minio: 
        condition: service_healthy
    environment:
      SERVICE_NAME: metastore
      HIVE_METASTORE_DB_TYPE: postgres
      HIVE_METASTORE_USER: hive
      HIVE_METASTORE_PASSWORD: hive
      HIVE_METASTORE_DB_HOST: postgres-metastore
      HIVE_METASTORE_DB_NAME: metastore
      DB_DRIVER: postgres
    volumes:
      - ./infra/hive-conf:/opt/hive/conf
      - ./infra/jars/postgresql-42.6.2.jar:/opt/hive/lib/postgresql-42.6.2.jar
      - ./infra/jars/hadoop-aws-3.3.4.jar:/opt/hive/lib/hadoop-aws-3.3.4.jar
      - ./infra/jars/aws-java-sdk-bundle-1.12.367.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.367.jar
    ports:
      - "9083:9083"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9083/health"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 5s
    networks:
      - mongodb_platform

  spark-master:
    image: spark:3.5.6-scala2.12-java11-python3-r-ubuntu
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_MEMORY=1G
      - SPARK_MASTER_CORES=1
      - SPARK_NO_DAEMONIZE=false
    volumes:
      - ./infra/hive-conf/hive-site.xml:/opt/spark/conf
      - ./infra/jars/postgresql-42.6.2.jar:/opt/spark/jars/postgresql-42.6.2.jar
      - ./infra/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/hadoop-aws-3.3.4.jar
      - ./infra/jars/aws-java-sdk-bundle-1.12.367.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar
      - ./infra/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar:/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
      - ./infra/jars/delta-spark_2.12-3.1.0.jar:/opt/spark/jars/delta-spark_2.12-3.1.0.jar
      - ./infra/jars/delta-storage-3.1.0.jar:/opt/spark/jars/delta-storage-3.1.0.jar
      - ./infra/jars/ojdbc8.jar:/opt/spark/jars/ojdbc8.jar
      - ./jobs:/opt/spark/jobs
      - ./config:/opt/spark/platform-config
      - ./infra/job-lib:/opt/spark/job-lib
      - ./src/business_logic/sql_transform:/opt/spark/sql-job
    command: /opt/spark/sbin/start-master.sh
    depends_on:
      - hive-metastore
    ports:
      - "7077:7077"
      - "8080:8080"
      - "4040:4040"
    networks:
      - mongodb_platform

  spark-worker:
    image: spark:3.5.6-scala2.12-java11-python3-r-ubuntu
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_NO_DAEMONIZE=false
    volumes:
      - ./infra/jars/postgresql-42.6.2.jar:/opt/spark/jars/postgresql-42.6.2.jar
      - ./infra/hive-conf/hive-site.xml:/opt/spark/conf
      - ./infra/jars/hadoop-aws-3.3.4.jar:/opt/spark/jars/hadoop-aws-3.3.4.jar
      - ./infra/jars/aws-java-sdk-bundle-1.12.367.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar
      - ./infra/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar:/opt/spark/jars/iceberg-spark-runtime-3.5_2.12-1.5.2.jar
    command: /opt/spark/sbin/start-worker.sh spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    networks:
      - mongodb_platform

  redis:
    image: redis:7
    container_name: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - mongodb_platform

  airflow-init:
    image: airflow-java
    container_name: airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://hive:hive@postgres-metastore:5432/metastore
      - AIRFLOW__CORE__FERNET_KEY=7ASnGStUOp03F0Dyv5S0ySTqzn45QbTzln0B4fktJDA=
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://hive:hive@postgres-metastore:5432/metastore
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - _PIP_ADDITIONAL_REQUIREMENTS=-r /opt/airflow/requirements_airflow.txt
      - AIRFLOW_ADMIN_PASSWORD=admin123
    volumes:
      - ./orchestration/dags:/opt/airflow/dags
      - ./orchestration/logs:/opt/airflow/logs
      - ./config/requirements_airflow.txt:/opt/airflow/requirements_airflow.txt
    command: >
      bash -c "pip install --user -r /opt/airflow/requirements_airflow.txt &&
      pip install dbt-spark[PyHive] &&
      echo 'Waiting for PostgreSQL to be ready...' &&
      sleep 5 &&
      airflow db migrate &&
      echo 'Setting Airflow Variable: processing_date' &&
      airflow variables set processing_date 2014-09-07 &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password "admin123""
    depends_on:
      - postgres-metastore
      - redis
    networks:
      - mongodb_platform

  airflow-webserver:
    image: airflow-java
    container_name: airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://hive:hive@postgres-metastore:5432/metastore
      - AIRFLOW__CORE__FERNET_KEY=7ASnGStUOp03F0Dyv5S0ySTqzn45QbTzln0B4fktJDA=
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://hive:hive@postgres-metastore:5432/metastore
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - _PIP_ADDITIONAL_REQUIREMENTS=-r /opt/airflow/requirements_airflow.txt
    volumes:
      - ./orchestration/dags:/opt/airflow/dags
      - ./orchestration/logs:/opt/airflow/logs
      - ./config/requirements_airflow.txt:/opt/airflow/requirements_airflow.txt

    ports:
      - "8900:8080"  # Airflow Web UI port
    depends_on:
      - airflow-init
      - postgres-metastore
      - redis
      - hive-metastore
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 5s
    networks:
      - mongodb_platform

  airflow-scheduler:
    image: airflow-java
    container_name: airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://hive:hive@postgres-metastore:5432/metastore
      - AIRFLOW__CORE__FERNET_KEY=7ASnGStUOp03F0Dyv5S0ySTqzn45QbTzln0B4fktJDA=
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://hive:hive@postgres-metastore:5432/metastore
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - _PIP_ADDITIONAL_REQUIREMENTS=-r /opt/airflow/requirements_airflow.txt

    volumes:
      - ./orchestration/dags:/opt/airflow/dags
      - ./orchestration/logs:/opt/airflow/logs
      - ./config/requirements_airflow.txt:/opt/airflow/requirements_airflow.txt
    depends_on:
      - airflow-init
      - airflow-webserver
      - redis
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8793/health"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 5s
    networks:
      - mongodb_platform

  airflow-worker:
    image: airflow-java
    container_name: airflow-worker
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://hive:hive@postgres-metastore:5432/metastore
      - AIRFLOW__CORE__FERNET_KEY=7ASnGStUOp03F0Dyv5S0ySTqzn45QbTzln0B4fktJDA=
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://hive:hive@postgres-metastore:5432/metastore
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=300
      - _PIP_ADDITIONAL_REQUIREMENTS=-r /opt/airflow/requirements_airflow.txt

    volumes:
      - ./orchestration/dags:/opt/airflow/dags
      - ./orchestration/logs:/opt/airflow/logs
      - ./config/requirements_airflow.txt:/opt/airflow/requirements_airflow.txt
    depends_on:
      - airflow-init
      - airflow-scheduler
      - airflow-webserver
      - redis
    command: celery worker
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8793/health"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 5s
    networks:
      - mongodb_platform



volumes:
  mongodb_data:
  minio_data:

networks:
  mongodb_platform:
